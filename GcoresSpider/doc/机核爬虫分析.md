# 机核爬虫分析：

	https://www.g-cores.com/articles/<article_id>
	exm:97627

## 清洗出来的内容

*　文章标题xpath：

	'.//div[@class="story_header_text"]/div[@class="story_header_text"]/h1[@class="story_title"]'

* 文章作者id：

	'.//a[@class="story_user"]'

href内容：`/users/<id>`

* 文章标签？xpath
	'.//div[@class="story_console"]/a' a.get_attribute('title')

* 文章正文xpath：
	'.//div[@class="story_elem story_elem-text "]/p'

也许可以统计文章的图片数 统计story_elem story_elem-image就可以了

* 文章点赞数：

	'.//strong[@id="j_original_likesNum"]'

*　文章评论数：

	'.//span[@class="j_commentsNum"]' 	# 直接从html中无法获取，需要提交js来获取评论数和评论内容

*　文章发布时间：

	'.//p[@class="story_info"]'



## 多线程爬虫

* 采用包：threading
* 采用queue队列，生成一个乱序list写入queue中，每次线程从队列中get一个id，进行爬取，直到队列为空


* 需要端口代理：

	user_agent ='Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0'
	headers = {'User-Agent':user_agent}
	proxy={'http':'http://%s:%S' % (ip, port)}

	# requests
	proxies = {
		pass
	}
	requests.get(url, proxies=proxies)

也可以存储多个user_agent到list中，每次用`random.sample(list, 1)[0]`获取其中一个user_agent作后续爬取

* requests使用了urllib3，默认的http-connection是keep-alive,需要在requests中设置关闭

	s = requests.session()
	s.keep_alive = False

	# 或者不使用connection:keep-alive
	request.get(url, headers={'Connection':'close'})

	# 或者在将response传入其他函数后r.close()


## 分析内容：
* 发布时间（小时/每周几/每个月）
* 主题（索/任/软/PC）
* 文章标题的关键词
* 文章内容的关键词
* 文章长度
* 图片数量
* 用户是管理员与否(这个需要后续进行追加)
* 栏目
* 点赞和评论数(主楼和子楼)
* 评论的关键词(积极和消极)
* 评论发布的时间

将上述进行组合

* 文章发布时间分布
* 文章的图片的数量和点赞数/评论的关系
* 文章的发布时间和点赞数/评论数的关系
* 文章栏目如何提取 -> 似乎只能从文章总页获取
* 用户发布文章频率 -> 应该从html中提取用户名一同存储 -> 管理员和非管理员的区分
* 评论者喜欢在什么时候发布评论(小时)(绝对时间)
* 文章的评论时间曲线(相对时间)

采取28分，将点赞数和评论数为前20%的文章判断为热点文章(?)

* 发布时间与热点文章
* 热点文章的关键词(以年限区分)
* 专栏与热点文章
